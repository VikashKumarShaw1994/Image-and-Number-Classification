<html>
<head>
<title>naiveBayes.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
naiveBayes.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">collections</span>
<span class="s0">from </span><span class="s1">array </span><span class="s0">import </span><span class="s1">array</span>

<span class="s0">import </span><span class="s1">dataClassifier</span>
<span class="s0">import </span><span class="s1">util</span>
<span class="s0">import </span><span class="s1">classificationMethod</span>
<span class="s0">import </span><span class="s1">math</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>


<span class="s0">class </span><span class="s1">NaiveBayesClassifier(classificationMethod.ClassificationMethod):</span>
    <span class="s2">&quot;&quot;&quot; 
  See the project description for the specifications of the Naive Bayes classifier. 
 
  Note that the variable 'datum' in this code refers to a counter of features 
  (not to a raw samples.Datum). 
  &quot;&quot;&quot;</span>
    <span class="s1">Test=</span><span class="s0">None</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">legalLabels):</span>
        <span class="s1">self.legalLabels = legalLabels</span>
        <span class="s1">self.type = </span><span class="s3">&quot;naivebayes&quot;</span>
        <span class="s1">self.k = </span><span class="s4">1  </span><span class="s5"># this is the smoothing parameter, ** use it in your train method **</span>
        <span class="s1">self.automaticTuning = </span><span class="s0">False  </span><span class="s5"># Look at this flag to decide whether to choose k automatically ** use this in your train method **</span>

    <span class="s0">def </span><span class="s1">setSmoothing(self</span><span class="s0">, </span><span class="s1">k):</span>
        <span class="s2">&quot;&quot;&quot; 
    This is used by the main method to change the smoothing parameter before training. 
    Do not modify this method. 
    &quot;&quot;&quot;</span>
        <span class="s1">self.k = k</span>

    <span class="s0">def </span><span class="s1">train(self</span><span class="s0">, </span><span class="s1">trainingData</span><span class="s0">, </span><span class="s1">trainingLabels</span><span class="s0">, </span><span class="s1">validationData</span><span class="s0">, </span><span class="s1">validationLabels):</span>
        <span class="s2">&quot;&quot;&quot; 
    Outside shell to call your method. Do not modify this method. 
    &quot;&quot;&quot;</span>

        <span class="s1">self.features = trainingData[</span><span class="s4">0</span><span class="s1">].keys()  </span><span class="s5"># this could be useful for your code later...</span>

        <span class="s0">if </span><span class="s1">(self.automaticTuning):</span>
            <span class="s1">kgrid = [</span><span class="s4">0.001</span><span class="s0">, </span><span class="s4">0.01</span><span class="s0">, </span><span class="s4">0.05</span><span class="s0">, </span><span class="s4">0.1</span><span class="s0">, </span><span class="s4">0.5</span><span class="s0">, </span><span class="s4">1</span><span class="s0">, </span><span class="s4">5</span><span class="s0">, </span><span class="s4">10</span><span class="s0">, </span><span class="s4">20</span><span class="s0">, </span><span class="s4">50</span><span class="s1">]</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">kgrid = [self.k]</span>

        <span class="s1">self.trainAndTune(trainingData</span><span class="s0">, </span><span class="s1">trainingLabels</span><span class="s0">, </span><span class="s1">validationData</span><span class="s0">, </span><span class="s1">validationLabels</span><span class="s0">, </span><span class="s1">kgrid)</span>

    <span class="s0">def </span><span class="s1">trainAndTune(self</span><span class="s0">, </span><span class="s1">trainingData</span><span class="s0">, </span><span class="s1">trainingLabels</span><span class="s0">, </span><span class="s1">validationData</span><span class="s0">, </span><span class="s1">validationLabels</span><span class="s0">, </span><span class="s1">kgrid):</span>
        <span class="s2">&quot;&quot;&quot; 
    Trains the classifier by collecting counts over the training data, and 
    stores the Laplace smoothed estimates so that they can be used to classify. 
    Evaluate each value of k in kgrid to choose the smoothing parameter 
    that gives the best accuracy on the held-out validationData. 
 
    trainingData and validationData are lists of feature Counters.  The corresponding 
    label lists contain the correct label for each datum. 
 
    To get the list of all possible features or labels, use self.features and 
    self.legalLabels. 
    &quot;&quot;&quot;</span>

        <span class="s3">&quot;*** YOUR CODE HERE ***&quot;</span>
        <span class="s1">row=</span><span class="s4">0</span>
        <span class="s1">col=</span><span class="s4">0</span>
        <span class="s1">grid=</span><span class="s4">1</span>

        <span class="s1">trainingLabels = trainingLabels + validationLabels</span>
        <span class="s1">trainingData = trainingData + validationData</span>
        <span class="s1">P_Y_Count = collections.Counter(trainingLabels)</span>
        <span class="s0">global </span><span class="s1">Test</span>
        <span class="s0">if </span><span class="s1">len(P_Y_Count.keys())==</span><span class="s4">2</span><span class="s1">:</span>

            <span class="s1">Test=</span><span class="s3">&quot;Face&quot;</span>
            <span class="s1">row=dataClassifier.FACE_DATUM_HEIGHT</span>
            <span class="s1">col=dataClassifier.FACE_DATUM_WIDTH</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">Test = </span><span class="s3">&quot;Digit&quot;</span>
            <span class="s1">row = dataClassifier.DIGIT_DATUM_HEIGHT</span>
            <span class="s1">col = dataClassifier.DIGIT_DATUM_WIDTH</span>



        <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">P_Y_Count.keys():</span>
            <span class="s1">P_Y_Count[k] = P_Y_Count[k] / (len(trainingLabels))</span>
        <span class="s1">sec = dict()</span>

        <span class="s0">for </span><span class="s1">keys </span><span class="s0">in </span><span class="s1">P_Y_Count.keys():  </span><span class="s5"># For every item we create a new dict</span>
            <span class="s1">sec[keys] = collections.defaultdict(list)  </span><span class="s5"># Create the sec of default dictionary list</span>

        <span class="s0">for </span><span class="s1">x</span><span class="s0">, </span><span class="s1">prob </span><span class="s0">in </span><span class="s1">P_Y_Count.items():</span>
            <span class="s1">first = list()</span>
            <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">ptr </span><span class="s0">in </span><span class="s1">enumerate(trainingLabels):  </span><span class="s5"># go through the traningLabels and check the indexs and append</span>
                <span class="s0">if </span><span class="s1">x == ptr:  </span><span class="s5"># Check the index</span>
                    <span class="s1">first.append(i)</span>

            <span class="s1">second = list()</span>

            <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">first:  </span><span class="s5"># Second is list that will contain training data based on labels</span>
                <span class="s1">second.append(trainingData[i])</span>
            <span class="s1">keys = list()</span>
            <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">range(len(second)):  </span><span class="s5"># Now we populate the dictionary with the correct label and the data</span>
                <span class="s1">a = np.array(list(second[y].values()))</span>
                <span class="s1">b = np.reshape(a</span><span class="s0">, </span><span class="s1">(row</span><span class="s0">, </span><span class="s1">col))</span>
                <span class="s1">key = list()</span>
                <span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">row</span><span class="s0">, </span><span class="s1">grid):</span>
                    <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">col</span><span class="s0">, </span><span class="s1">grid):</span>
                        <span class="s1">key.append((b[z:z + grid</span><span class="s0">, </span><span class="s1">y:y + grid]))</span>

                <span class="s1">keys = list()</span>
                <span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">key:</span>
                    <span class="s1">keys.append(np.sum(a))</span>
                <span class="s0">for </span><span class="s1">r</span><span class="s0">, </span><span class="s1">val </span><span class="s0">in </span><span class="s1">enumerate(keys):</span>
                    <span class="s1">sec[x][r].append(val)</span>

        <span class="s1">count = [a </span><span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">P_Y_Count]  </span><span class="s5"># Get the total count</span>

        <span class="s5"># for x in count:</span>
        <span class="s5">#  for k, ptr in second[0].items():</span>
        <span class="s5">#   sec[x][k[1]] = self.check(sec[x][k[1]])  # Get the probabilties for Naive Bayes</span>

        <span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">ptr </span><span class="s0">in </span><span class="s1">sec.items():</span>
            <span class="s1">x = ptr.keys()</span>
            <span class="s1">y = ptr.values()</span>
            <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">j </span><span class="s0">in </span><span class="s1">zip((x)</span><span class="s0">, </span><span class="s1">(y)):</span>
                <span class="s1">sec[k][i] = self.check(j)</span>

        <span class="s1">self.intial = P_Y_Count  </span><span class="s5"># Update the P_Y_Count</span>
        <span class="s1">self.count = count  </span><span class="s5"># Update the count</span>
        <span class="s1">self.sec = sec  </span><span class="s5"># Update the second list with the training label and training data</span>

    <span class="s5"># util.raiseNotDefined()</span>

    <span class="s0">def </span><span class="s1">check(self</span><span class="s0">, </span><span class="s1">out):</span>
        <span class="s1">prob = dict(collections.Counter(out))</span>
        <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">prob.keys():</span>
            <span class="s1">prob[k] = prob[k] / float(len(out))</span>
        <span class="s0">return </span><span class="s1">prob</span>

    <span class="s0">def </span><span class="s1">classify(self</span><span class="s0">, </span><span class="s1">testData):</span>
        <span class="s2">&quot;&quot;&quot; 
    Classify the data based on the posterior distribution over labels. 
 
    You shouldn't modify this method. 
    &quot;&quot;&quot;</span>
        <span class="s1">guesses = []</span>
        <span class="s1">self.posteriors = []  </span><span class="s5"># Log posteriors are stored for later data analysis (autograder).</span>
        <span class="s0">for </span><span class="s1">datum </span><span class="s0">in </span><span class="s1">testData:</span>
            <span class="s1">posterior = self.calculateLogJointProbabilities(datum)</span>
            <span class="s1">guesses.append(posterior.argMax())</span>
            <span class="s1">self.posteriors.append(posterior)</span>
        <span class="s0">return </span><span class="s1">guesses</span>

    <span class="s0">def </span><span class="s1">calculateLogJointProbabilities(self</span><span class="s0">, </span><span class="s1">datum):</span>
        <span class="s2">&quot;&quot;&quot; 
    Returns the log-joint distribution over legal labels and the datum. 
    Each log-probability should be stored in the log-joint counter, e.g. 
    logJoint[3] = &lt;Estimate of log( P(Label = 3, datum) )&gt; 
    &quot;&quot;&quot;</span>
        <span class="s1">logJoint = util.Counter()</span>

        <span class="s3">&quot;*** YOUR CODE HERE ***&quot;</span>
        <span class="s1">row = </span><span class="s4">0</span>
        <span class="s1">col = </span><span class="s4">0</span>
        <span class="s1">grid = </span><span class="s4">1</span>
        <span class="s0">global </span><span class="s1">Test</span>
        <span class="s0">if </span><span class="s1">Test == </span><span class="s3">&quot;Face&quot;</span><span class="s1">:</span>
            <span class="s1">row = dataClassifier.FACE_DATUM_HEIGHT</span>
            <span class="s1">col = dataClassifier.FACE_DATUM_WIDTH</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">row = dataClassifier.DIGIT_DATUM_HEIGHT</span>
            <span class="s1">col = dataClassifier.DIGIT_DATUM_WIDTH</span>
        <span class="s1">a = np.array(list(datum.values()))</span>
        <span class="s1">b = np.reshape(a</span><span class="s0">, </span><span class="s1">(row</span><span class="s0">, </span><span class="s1">col))</span>
        <span class="s1">key = list()</span>
        <span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">row</span><span class="s0">, </span><span class="s1">grid):</span>
            <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">0</span><span class="s0">, </span><span class="s1">col</span><span class="s0">, </span><span class="s1">grid):</span>
                <span class="s1">key.append((b[z:z + grid</span><span class="s0">, </span><span class="s1">y:y + grid]))</span>
        <span class="s1">keys = list()</span>
        <span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">key:</span>
            <span class="s1">keys.append(np.sum(a))</span>

        <span class="s1">n = dict()</span>
        <span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">self.count:</span>
            <span class="s1">probs = self.intial[x]  </span><span class="s5"># Get the probabilty</span>
            <span class="s1">probs=math.log(probs)</span>

            <span class="s1">nf = self.sec.get(x)</span>
            <span class="s0">for </span><span class="s1">k</span><span class="s0">, </span><span class="s1">ptr </span><span class="s0">in </span><span class="s1">enumerate(keys):</span>
                <span class="s5"># Get the data we need from the sec dict</span>
                <span class="s0">if </span><span class="s1">nf.get(k).get(ptr) == </span><span class="s0">None</span><span class="s1">:</span>
                    <span class="s1">probs = probs + math.log(</span><span class="s4">0.000001</span><span class="s1">)</span>
                    <span class="s0">continue</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s1">p = nf.get(k).get(ptr)</span>
                    <span class="s1">probs = probs + math.log(p)  </span><span class="s5"># Calculate the probability</span>

            <span class="s1">logJoint[x] = probs  </span><span class="s5"># Add the new probability back to the log Joint list</span>
        <span class="s5"># util.raiseNotDefined()</span>
        <span class="s1">m = max(logJoint.values())</span>
        <span class="s0">return </span><span class="s1">logJoint</span>

    <span class="s0">def </span><span class="s1">findHighOddsFeatures(self</span><span class="s0">, </span><span class="s1">label1</span><span class="s0">, </span><span class="s1">label2):</span>
        <span class="s2">&quot;&quot;&quot; 
    Returns the 100 best features for the odds ratio: 
            P(feature=1 | label1)/P(feature=1 | label2) 
    &quot;&quot;&quot;</span>
        <span class="s1">featuresOdds = []</span>

        <span class="s3">&quot;*** YOUR CODE HERE ***&quot;</span>
        <span class="s1">util.raiseNotDefined()</span>

        <span class="s0">return </span><span class="s1">featuresOdds</span>




</pre>
</body>
</html>